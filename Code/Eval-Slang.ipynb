{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec002be",
   "metadata": {},
   "source": [
    "# Supplementary code package for 'Toward Informal Language Processing: Knowledge of Slang in Large Language Models'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4445b9-8f55-44a9-b6cc-30a6b6f58be3",
   "metadata": {},
   "source": [
    "By: [Zhewei Sun](https://www.cs.toronto.edu/~zheweisun/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcedd237",
   "metadata": {},
   "source": [
    "This notebook contains experiment code for the NAACL Paper 'Toward Informal Language Processing: Knowledge of Slang in Large Language Models'. Here, we show how the contributed dataset can be used to reproduce the main experiments in the paper. We include all code used to execute the experiments on BERT-like models (this includes BERT, RoBERTa, and XLNet) and illustrate example usage with BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2393bc6",
   "metadata": {},
   "source": [
    "To run this notebook, you will need the following Python packages:\n",
    "\n",
    "- nltk\n",
    "- numpy\n",
    "- pandas\n",
    "- pytorch\n",
    "- scipy\n",
    "- tqdm\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e201a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, namedtuple, Counter, defaultdict, OrderedDict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bcd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification, BertForTokenClassification\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaForSequenceClassification, RobertaForTokenClassification\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, XLNetForSequenceClassification, XLNetForTokenClassification\n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast, XLNetTokenizerFast\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c48e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to specify a GPU as needed. \n",
    "# torch.cuda.set_device(1)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2065fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(sw.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e65b1",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbefaa0",
   "metadata": {},
   "source": [
    "The dataset is included in the supplmentary data package. If you downloaded the entire package from the repo, the data can be found in *../Data/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3dae0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_os = pd.read_csv('../Data/slang_OpenSub.tsv', sep='\\t').fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1189c",
   "metadata": {},
   "source": [
    "All attributes attached to the data entries. See the paper and the README file in *../Data* for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac12d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SENTENCE', 'FULL_CONTEXT', 'SLANG_TERM', 'ANNOTATOR_CONFIDENCE',\n",
       "       'MOVIE_ID', 'SENT_ID', 'REGION', 'YEAR', 'DEFINITION_SENTENCE',\n",
       "       'DEFINITION_SOURCE_URL', 'LITERAL_PARAPHRASE_OF_SLANG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_os.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877a9402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7488"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace944a",
   "metadata": {},
   "source": [
    "We also load annotated negative sentences (i.e. sentences that do not contain slang) for slang detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86e82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_os_neg = pd.read_csv('../Data/slang_OpenSub_negatives.tsv', sep='\\t').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2bb256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_os_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b105e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sents = data_os['SENTENCE'].values\n",
    "data_slang = data_os['SLANG_TERM'].values\n",
    "data_literal = data_os['LITERAL_PARAPHRASE_OF_SLANG'].values\n",
    "data_region = data_os['REGION'].values\n",
    "data_year = data_os['YEAR'].values\n",
    "data_conf = data_os['ANNOTATOR_CONFIDENCE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee81f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sents_neg = data_os_neg['SENTENCE'].values\n",
    "data_region_neg = data_os_neg['REGION'].values\n",
    "data_year_neg = data_os_neg['YEAR'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de255ddd",
   "metadata": {},
   "source": [
    "The following indices record the exact set of sentences used in the paper's evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7fbc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_llm_inds = np.load('../Data/slang_llm_inds.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc33494",
   "metadata": {},
   "source": [
    "## Slang Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d6a36",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d878efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '!\\'\"#$%&()\\*\\+,-\\./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "re_punc = re.compile(r\"[\"+punctuations+r\"]+\")\n",
    "re_punc2 = re.compile(r\"[,.!;:&<>-]+\")\n",
    "re_punc_space = re.compile(r\" [\"+punctuations+r\"]+ \")\n",
    "re_space = re.compile(r\" +\")\n",
    "\n",
    "def add_space(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return ' '+match_obj.group()+' '\n",
    "    \n",
    "def add_space_trail(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return match_obj.group()+' '\n",
    "    \n",
    "def remove_space(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return match_obj.group()[1:-1]\n",
    "\n",
    "sents_ind = []\n",
    "sents_mask = []\n",
    "for i in range(len(data_sents)):\n",
    "    \n",
    "    if data_conf[i] < 2:\n",
    "        continue\n",
    "    \n",
    "    tokens = re_space.sub(' ', re_punc.sub(add_space, data_sents[i])).split(' ')\n",
    "    slang_pos = []\n",
    "            \n",
    "    for j in range(len(tokens)):\n",
    "        token = tokens[j]\n",
    "        if token.lower() == data_slang[i].lower():\n",
    "            slang_pos.append(j)\n",
    "            \n",
    "    if len(slang_pos) == 0:\n",
    "        tokens = re_space.sub(' ', re_punc.sub('', data_sents[i])).split(' ')\n",
    "        slang_pos = []\n",
    "        slang_nop = re_punc.sub('', data_slang[i])\n",
    "\n",
    "        for j in range(len(tokens)):\n",
    "            token = tokens[j]\n",
    "            if token.lower() == slang_nop.lower():\n",
    "                slang_pos.append(j)\n",
    "        \n",
    "    if len(slang_pos) == 1:\n",
    "        sents_ind.append(i)\n",
    "        t = tokens.copy()\n",
    "        for p in slang_pos:\n",
    "            t[p] = '[MASK]'\n",
    "        sents_mask.append(re_punc2.sub(add_space_trail, re_punc_space.sub(remove_space, ' '.join(t))).strip())\n",
    "            \n",
    "sents_ind = np.asarray(sents_ind)\n",
    "\n",
    "words_slang = data_slang[sents_ind]\n",
    "words_literal = data_literal[sents_ind]\n",
    "regions = data_region[sents_ind]\n",
    "confs = data_conf[sents_ind]\n",
    "\n",
    "sents_mask_pos = [sents_mask[i].index('[MASK]') for i in range(len(sents_mask))]\n",
    "sents_slang = np.asarray([sents_mask[i].replace('[MASK]', words_slang[i]) for i in range(len(sents_mask))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2d0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_neg_sample = slang_llm_inds['detect_neg']\n",
    "\n",
    "sents_neg = []\n",
    "\n",
    "for i in range(len(ind_neg_sample)):\n",
    "    tokens = re_space.sub(' ', re_punc.sub(add_space, data_sents_neg[ind_neg_sample[i]])).split(' ')\n",
    "    sents_neg.append(re_punc2.sub(add_space_trail, re_punc_space.sub(remove_space, ' '.join(tokens))).strip())\n",
    "    \n",
    "sents_neg = np.asarray(sents_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093fb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sample = len(sents_slang)*2\n",
    "\n",
    "data_perm = slang_llm_inds['detect']\n",
    "sents_all = np.concatenate([sents_neg, sents_slang])\n",
    "sents_detect = sents_all[data_perm]\n",
    "\n",
    "pivot_tr = int(np.floor(N_sample*0.8))\n",
    "pivot_dev = int(np.floor(N_sample*0.85))\n",
    "\n",
    "tr_sents = sents_detect[:pivot_tr]\n",
    "dev_sents = sents_detect[pivot_tr:pivot_dev]\n",
    "te_sents = sents_detect[pivot_dev:]\n",
    "\n",
    "train_inds = data_perm[:pivot_tr]\n",
    "dev_inds = data_perm[pivot_tr:pivot_dev]\n",
    "test_inds = data_perm[pivot_dev:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c103bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_detect = np.asarray([0]*len(sents_slang)+[1]*len(sents_slang))[data_perm]\n",
    "\n",
    "tr_labels_detect = labels_detect[:pivot_tr]\n",
    "dev_labels_detect = labels_detect[pivot_tr:pivot_dev]\n",
    "te_labels_detect = labels_detect[pivot_dev:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300aac7f",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b1d5c",
   "metadata": {},
   "source": [
    "#### Sentence-level Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c6afec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_detect_classifier(tr_sents, tr_labels, dev_sents, dev_labels, save_path, model_type='bert', num_labels=2, N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False):\n",
    "\n",
    "    if model_type.lower() == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-large-cased', num_labels=num_labels).to(device)\n",
    "        n_save = 4\n",
    "    elif model_type.lower() == 'roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels).to(device)\n",
    "        n_save = 4\n",
    "    elif model_type.lower() == 'xlnet':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-large-cased\")\n",
    "        model = XLNetForSequenceClassification.from_pretrained(\"xlnet-large-cased\", num_labels=num_labels).to(device)\n",
    "        n_save = 4\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return\n",
    "\n",
    "    for name, param in list(model.named_parameters())[:-n_save]:\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5)\n",
    "\n",
    "    best_dev = np.inf\n",
    "    \n",
    "    if verbose:\n",
    "        range_fn = range\n",
    "        range_fn2 = trange\n",
    "    else:\n",
    "        range_fn = trange\n",
    "        range_fn2 = range\n",
    "\n",
    "    for e in range_fn(N_EPOCHS):\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[Training Epoch - %d]\" % (e+1))\n",
    "            print(\"\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        tr_shuf = np.random.permutation(tr_sents.shape[0])\n",
    "\n",
    "        loss_total, loss_total_dev = 0, 0\n",
    "\n",
    "        for b in range_fn2(0, tr_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "            b_end = min(tr_sents.shape[0], b+BATCH_SIZE)\n",
    "            batch_sample = tr_shuf[b:b_end]\n",
    "\n",
    "            tr_sents_batch = tr_sents[batch_sample]\n",
    "            tr_labels_batch = tr_labels[batch_sample]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tr_ids_batch = tokenizer(list(tr_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "            labels = torch.tensor(tr_labels_batch).to(device)\n",
    "\n",
    "            loss = model(**tr_ids_batch, labels=labels).loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total += loss.item() * batch_sample.shape[0]\n",
    "\n",
    "            del tr_ids_batch, labels, loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training Loss: %.3f\" % (loss_total / tr_sents.shape[0]))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        for b in range_fn2(0, dev_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "            b_end = min(dev_sents.shape[0], b+BATCH_SIZE)\n",
    "            batch_sample = np.arange(b, b_end)\n",
    "\n",
    "            dev_sents_batch = dev_sents[batch_sample]\n",
    "            dev_labels_batch = dev_labels[batch_sample]\n",
    "\n",
    "            dev_ids_batch = tokenizer(list(dev_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "            labels = torch.tensor(dev_labels_batch).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss = model(**dev_ids_batch, labels=labels).loss\n",
    "\n",
    "            loss_total_dev += loss.item() * batch_sample.shape[0]\n",
    "\n",
    "            del dev_ids_batch, labels, loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Dev Loss: %.3f\" % (loss_total_dev / dev_sents.shape[0]))\n",
    "        \n",
    "        scheduler.step(loss_total_dev)\n",
    "\n",
    "        if loss_total_dev < best_dev:\n",
    "\n",
    "            best_dev = loss_total_dev\n",
    "            if verbose:\n",
    "                print(\"Best dev loss so far, saving model...\")\n",
    "            d = OrderedDict()\n",
    "            for name, param in list(model.state_dict().items())[-n_save:]:\n",
    "                d[name] = param\n",
    "            torch.save(d, save_path)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def test_detect_classifier(te_sents, save_path, model_type='bert', num_labels=6, BATCH_SIZE = 20):\n",
    "    \n",
    "    if model_type.lower() == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-large-cased', num_labels=num_labels).to(device)\n",
    "    elif model_type.lower() == 'roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels).to(device)\n",
    "    elif model_type.lower() == 'xlnet':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-large-cased\")\n",
    "        model = XLNetForSequenceClassification.from_pretrained(\"xlnet-large-cased\", num_labels=num_labels).to(device)\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return\n",
    "    \n",
    "    _ = model.load_state_dict(torch.load(save_path), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for b in trange(0, te_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "        b_end = min(te_sents.shape[0], b+BATCH_SIZE)\n",
    "        batch_sample = np.arange(b, b_end)\n",
    "\n",
    "        te_sents_batch = te_sents[batch_sample]\n",
    "\n",
    "        te_ids_batch = tokenizer(list(te_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(**te_ids_batch).logits.argmax(axis=1)\n",
    "\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "\n",
    "        del te_ids_batch, pred\n",
    "\n",
    "    return np.asarray(preds)\n",
    "\n",
    "def run_detect_experiment(tr_sents, tr_labels, dev_sents, dev_labels, te_sents, te_labels, save_path, num_labels=6, num_exp=20, model_type='bert', N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False):\n",
    "    \n",
    "    res = []\n",
    "    acc = []\n",
    "    \n",
    "    for e in range(num_exp):\n",
    "        print(\"[Experiment - %d]\" % (e+1))\n",
    "        print(\"\")\n",
    "        \n",
    "        path = save_path+'_'+model_type+'_'+str(e+1)+'.pt'\n",
    "        \n",
    "        train_detect_classifier(tr_sents, tr_labels, dev_sents, dev_labels, save_path=path, model_type=model_type, num_labels=num_labels, N_EPOCHS=N_EPOCHS, BATCH_SIZE=BATCH_SIZE, verbose=verbose)\n",
    "        preds = test_detect_classifier(te_sents, path, model_type, num_labels=num_labels)\n",
    "        \n",
    "        res.append(preds)\n",
    "        acc.append(np.sum(preds==te_labels))\n",
    "        \n",
    "    return np.stack(res), np.asarray(acc)\n",
    "\n",
    "def compute_detect_result(preds, te_labels):\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    res['acc'] = np.mean([compute_acc_detect(pred, te_labels) for pred in preds])\n",
    "    \n",
    "    res['prec'], res['recall'], res['f1'] = np.mean(np.asarray([compute_f1_detect(pred, te_labels) for pred in preds]), axis=0)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def compute_acc_detect(preds, te_labels):\n",
    "    return np.sum(preds==te_labels) / len(preds)\n",
    "\n",
    "def compute_f1_detect(preds, te_labels):\n",
    "    \n",
    "    TP = np.sum(preds[te_labels==1]==1)\n",
    "    FP = np.sum(preds[te_labels==0]==1)\n",
    "    FN = np.sum(preds[te_labels==1]==0)\n",
    "    \n",
    "    prec = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = TP / (TP + 0.5*(FP + FN))\n",
    "    \n",
    "    return prec, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe54d17",
   "metadata": {},
   "source": [
    "Here, we run one experiment with BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d468dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment - 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [01:45<00:00, 10.60s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 29/29 [00:01<00:00, 15.89it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = '../Results/detect_sent/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "res_detect_bert, acc = run_detect_experiment(tr_sents, tr_labels_detect, dev_sents, dev_labels_detect, te_sents, te_labels_detect, save_path=save_path+'detect_sent', num_exp=1, model_type='bert', num_labels=2, N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90a3bc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7822299651567944,\n",
       " 'prec': 0.7235294117647059,\n",
       " 'recall': 0.8880866425992779,\n",
       " 'f1': 0.7974068071312804}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_detect_result(res_detect_bert, te_labels_detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1ab60",
   "metadata": {},
   "source": [
    "#### Word-level Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdc79762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels(input_tokens, sample_inds, tokenizer, model_type='bert', test_mode=False):\n",
    "    \n",
    "    input_ids = input_tokens.input_ids\n",
    "    \n",
    "    labels = np.zeros(input_ids.shape, dtype=np.int64)\n",
    "\n",
    "    for i in range(len(sample_inds)):\n",
    "\n",
    "        ind = sample_inds[i]\n",
    "        token_len = len(tokenizer.encode(sents_all[ind]))\n",
    "\n",
    "        if model_type == 'bert':\n",
    "            labels[i][0] = -100\n",
    "            labels[i][token_len-1:] = -100\n",
    "        elif model_type == 'roberta':\n",
    "            labels[i][0] = -100\n",
    "            labels[i][token_len-1:] = -100\n",
    "        elif model_type == 'xlnet':\n",
    "            labels[i][-2:] = -100\n",
    "            labels[i][:input_ids.shape[1]-token_len] = -100\n",
    "\n",
    "        if ind >= N_sample // 2:\n",
    "\n",
    "            ind = ind - N_sample // 2\n",
    "            \n",
    "            if model_type == 'bert':\n",
    "                id_word = tokenizer.encode(words_slang[ind])[1:-1]\n",
    "                m_start = len(tokenizer.encode(sents_slang[ind][:sents_mask_pos[ind]]))-1\n",
    "            elif model_type == 'roberta':\n",
    "                if sents_mask_pos[i] == 0:\n",
    "                    id_word = tokenizer.encode(words_slang[ind])[1:-1]\n",
    "                else:\n",
    "                    id_word = tokenizer.encode(' '+words_slang[ind])[1:-1]\n",
    "                m_start = len(tokenizer.encode(sents_slang[ind][:sents_mask_pos[ind]].strip()))-1\n",
    "            elif model_type == 'xlnet':\n",
    "                id_word = tokenizer.encode(words_slang[ind])[:-2]\n",
    "                m_start = input_ids.shape[1] - token_len\n",
    "                m_start += len(tokenizer.encode(sents_slang[ind][:sents_mask_pos[ind]]))-2\n",
    "            \n",
    "            for j in range(len(id_word)):\n",
    "                if j == 0:\n",
    "                    labels[i][m_start+j] = 1\n",
    "                else:\n",
    "                    labels[i][m_start+j] = 2\n",
    "        \n",
    "        # During testing, we don't consider subsequent subword tokens in any word so that the total number of tokens evaluated is consistent across models.\n",
    "        \n",
    "        if test_mode:\n",
    "            \n",
    "            word_ids = input_tokens.word_ids(i)\n",
    "            seen_ids = set()\n",
    "            \n",
    "            assert len(word_ids) == len(labels[i])\n",
    "            \n",
    "            for j in range(len(word_ids)):\n",
    "                if word_ids[j] is not None:\n",
    "                    if word_ids[j] not in seen_ids:\n",
    "                        seen_ids.add(word_ids[j])\n",
    "                    else:\n",
    "                        labels[i][j] = -1\n",
    "\n",
    "    return torch.tensor(labels)\n",
    "\n",
    "def train_ident_classifier(tr_sents, dev_sents, save_path, model_type='bert', num_labels=3, N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False):\n",
    "\n",
    "    if model_type.lower() == 'bert':\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-cased\")\n",
    "        model = BertForTokenClassification.from_pretrained('bert-large-cased', num_labels=num_labels).to(device)\n",
    "        n_save = 2\n",
    "    elif model_type.lower() == 'roberta':\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\")\n",
    "        model = RobertaForTokenClassification.from_pretrained(\"roberta-large\", num_labels=num_labels).to(device)\n",
    "        n_save = 2\n",
    "    elif model_type.lower() == 'xlnet':\n",
    "        tokenizer = XLNetTokenizerFast.from_pretrained(\"xlnet-large-cased\")\n",
    "        model = XLNetForTokenClassification.from_pretrained(\"xlnet-large-cased\", num_labels=num_labels).to(device)\n",
    "        n_save = 2\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return\n",
    "\n",
    "    for name, param in list(model.named_parameters())[:-n_save]:\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "    # Training loop\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5)\n",
    "\n",
    "    best_dev = np.inf\n",
    "\n",
    "    if verbose:\n",
    "        range_fn = range\n",
    "        range_fn2 = trange\n",
    "    else:\n",
    "        range_fn = trange\n",
    "        range_fn2 = range\n",
    "\n",
    "    for e in range_fn(N_EPOCHS):\n",
    "\n",
    "        if verbose:\n",
    "            print(\"[Training Epoch - %d]\" % (e+1))\n",
    "            print(\"\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        tr_shuf = np.random.permutation(tr_sents.shape[0])\n",
    "\n",
    "        loss_total, loss_total_dev = 0, 0\n",
    "\n",
    "        for b in range_fn2(0, tr_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "            b_end = min(tr_sents.shape[0], b+BATCH_SIZE)\n",
    "            batch_sample = tr_shuf[b:b_end]\n",
    "\n",
    "            tr_sents_batch = tr_sents[batch_sample]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tr_ids_batch = tokenizer(list(tr_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "            labels = align_labels(tr_ids_batch, train_inds[batch_sample], tokenizer, model_type).to(device)\n",
    "\n",
    "            loss = model(**tr_ids_batch, labels=labels).loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total += loss.item() * batch_sample.shape[0]\n",
    "\n",
    "            del tr_ids_batch, labels, loss\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training Loss: %.3f\" % (loss_total / tr_sents.shape[0]))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        for b in range_fn2(0, dev_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "            b_end = min(dev_sents.shape[0], b+BATCH_SIZE)\n",
    "            batch_sample = np.arange(b, b_end)\n",
    "\n",
    "            dev_sents_batch = dev_sents[batch_sample]\n",
    "\n",
    "            dev_ids_batch = tokenizer(list(dev_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "            labels = align_labels(dev_ids_batch, dev_inds[batch_sample], tokenizer, model_type).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss = model(**dev_ids_batch, labels=labels).loss\n",
    "\n",
    "            loss_total_dev += loss.item() * batch_sample.shape[0]\n",
    "\n",
    "            del dev_ids_batch, labels, loss\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dev Loss: %.3f\" % (loss_total_dev / dev_sents.shape[0]))\n",
    "\n",
    "        scheduler.step(loss_total_dev)\n",
    "\n",
    "        if loss_total_dev < best_dev:\n",
    "\n",
    "            best_dev = loss_total_dev\n",
    "            if verbose:\n",
    "                print(\"Best dev loss so far, saving model...\")\n",
    "            d = OrderedDict()\n",
    "            for name, param in list(model.state_dict().items())[-n_save:]:\n",
    "                d[name] = param\n",
    "            torch.save(d, save_path)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_ident_classifier(te_sents, save_path, model_type='bert', num_labels=3, BATCH_SIZE = 20):\n",
    "    \n",
    "    if model_type.lower() == 'bert':\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-cased\")\n",
    "        model = BertForTokenClassification.from_pretrained('bert-large-cased', num_labels=num_labels).to(device)\n",
    "    elif model_type.lower() == 'roberta':\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\")\n",
    "        model = RobertaForTokenClassification.from_pretrained(\"roberta-large\", num_labels=num_labels).to(device)\n",
    "    elif model_type.lower() == 'xlnet':\n",
    "        tokenizer = XLNetTokenizerFast.from_pretrained(\"xlnet-large-cased\")\n",
    "        model = XLNetForTokenClassification.from_pretrained(\"xlnet-large-cased\", num_labels=num_labels).to(device)\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return\n",
    "    \n",
    "    _ = model.load_state_dict(torch.load(save_path), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    gt_labels = []\n",
    "\n",
    "    for b in trange(0, te_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "        b_end = min(te_sents.shape[0], b+BATCH_SIZE)\n",
    "        batch_sample = np.arange(b, b_end)\n",
    "\n",
    "        te_sents_batch = te_sents[batch_sample]\n",
    "\n",
    "        te_ids_batch = tokenizer(list(te_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "        labels = align_labels(te_ids_batch, test_inds[batch_sample], tokenizer, model_type, test_mode=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(**te_ids_batch).logits.argmax(-1)\n",
    "\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        gt_labels.extend(labels.numpy())\n",
    "\n",
    "        del te_ids_batch, pred\n",
    "\n",
    "    return np.asarray(preds, dtype=object), np.asarray(gt_labels, dtype=object)\n",
    "\n",
    "def run_ident_experiment(tr_sents, dev_sents, te_sents, save_path, num_labels=3, num_exp=20, model_type='bert', N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for e in range(num_exp):\n",
    "        print(\"[Experiment - %d]\" % (e+1))\n",
    "        print(\"\")\n",
    "        \n",
    "        path = save_path+'_'+model_type+'_'+str(e+1)+'.pt'\n",
    "        \n",
    "        train_ident_classifier(tr_sents, dev_sents, save_path=path, model_type=model_type, num_labels=num_labels, N_EPOCHS=N_EPOCHS, BATCH_SIZE=BATCH_SIZE, verbose=verbose)\n",
    "        preds, labels = test_ident_classifier(te_sents, path, model_type, num_labels=num_labels)\n",
    "        \n",
    "        res.append(preds)\n",
    "        \n",
    "    return res, labels\n",
    "\n",
    "def compute_ident_result(preds, labels):\n",
    "    \n",
    "    preds_flat = [[] for i in range(len(preds))]\n",
    "    labels_flat = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        label_mask = labels[i] != -100\n",
    "        labels_flat.extend(labels[i][label_mask])\n",
    "        for j in range(len(preds)):\n",
    "            preds_flat[j].extend(preds[j][i][label_mask])\n",
    "        \n",
    "    preds_flat = [np.asarray(a) for a in preds_flat]\n",
    "    labels_flat = np.asarray(labels_flat)\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    res['acc'] = np.mean([compute_acc_ident(pred, labels_flat) for pred in preds_flat])\n",
    "    \n",
    "    res['prec'], res['recall'], res['f1'] = np.mean(np.asarray([compute_f1_ident(pred, labels_flat) for pred in preds_flat]), axis=0)\n",
    " \n",
    "    \n",
    "    return res\n",
    "\n",
    "def compute_acc_ident(preds, labels):\n",
    "    return np.sum(preds==labels) / len(preds)\n",
    "\n",
    "def compute_f1_ident(preds, labels):\n",
    "\n",
    "    T_mask = np.any([labels==1, labels==2], axis=0)\n",
    "\n",
    "    TP = np.sum(preds[T_mask]==labels[T_mask])\n",
    "    FP = np.sum(np.any([preds[labels==0]==1, preds[labels==0]==2], axis=0))\n",
    "    FN = np.sum(preds[T_mask]!=labels[T_mask])\n",
    "    \n",
    "    prec = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = TP / (TP + 0.5*(FP + FN))\n",
    "    \n",
    "    return prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32866b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment - 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [01:44<00:00, 10.41s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 29/29 [00:01<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = '../Results/detect_word/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "res_ident_bert, te_labels_bert = run_ident_experiment(tr_sents, dev_sents, te_sents, save_path=save_path+'detect_word', num_labels=3, num_exp=1, model_type='bert', N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee8fb859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9235115167318557,\n",
       " 'prec': 0.7863247863247863,\n",
       " 'recall': 0.5974025974025974,\n",
       " 'f1': 0.6789667896678967}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ident_result(res_ident_bert, te_labels_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea78b9f",
   "metadata": {},
   "source": [
    "## Slang Source Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3508d",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5852c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(65488209)\n",
    "\n",
    "data_mask = data_literal != ''\n",
    "\n",
    "sents_all = data_sents\n",
    "slang_all = data_slang\n",
    "regions_all = data_region\n",
    "confs_all = data_conf\n",
    "\n",
    "punctuations = '!\\'\"#$%&()\\*\\+,-\\./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "re_punc = re.compile(r\"[\"+punctuations+r\"]+\")\n",
    "re_punc2 = re.compile(r\"[,.!;:&<>-]+\")\n",
    "re_punc_space = re.compile(r\" [\"+punctuations+r\"]+ \")\n",
    "re_space = re.compile(r\" +\")\n",
    "re_allpuncspace = re.compile(r\"^[\\s\"+punctuations+\"]+$\")\n",
    "\n",
    "def add_space(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return ' '+match_obj.group()+' '\n",
    "    \n",
    "def add_space_trail(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return match_obj.group()+' '\n",
    "    \n",
    "def remove_space(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return match_obj.group()[1:-1]\n",
    "    \n",
    "context_sents = [[] for i in range(2)]\n",
    "context_inds = [[] for i in range(2)]\n",
    "masked_slang_sents = [[] for i in range(2)]\n",
    "masked_random_sents = [[] for i in range(2)]\n",
    "gt_words = [[] for i in range(2)]\n",
    "\n",
    "for i in range(len(sents_all)):\n",
    "    \n",
    "    if confs_all[i] < 2:\n",
    "        continue\n",
    "    \n",
    "    if regions_all[i] == 'US':\n",
    "        region_tag = 0\n",
    "    elif regions_all[i] == 'UK':\n",
    "        region_tag = 1\n",
    "                \n",
    "    tokens = re_space.sub(' ', re_punc.sub(add_space, sents_all[i])).split(' ')\n",
    "    slang_pos = []\n",
    "    content_pos = []\n",
    "            \n",
    "    for j in range(len(tokens)):\n",
    "        token = tokens[j]\n",
    "        if token.lower() == slang_all[i].lower():\n",
    "            slang_pos.append(j)\n",
    "        elif token.lower() not in stopwords and re_allpuncspace.search(token) is None and len(token) > 0:\n",
    "            content_pos.append(j)\n",
    "            \n",
    "    if len(slang_pos) == 0:\n",
    "        tokens = re_space.sub(' ', re_punc.sub('', sents_all[i])).split(' ')\n",
    "        slang_pos = []\n",
    "        content_pos = []\n",
    "        slang_nop = re_punc.sub('', slang_all[i])\n",
    "\n",
    "        for j in range(len(tokens)):\n",
    "            token = tokens[j]\n",
    "            if token.lower() == slang_nop.lower():\n",
    "                slang_pos.append(j)\n",
    "            elif token.lower() not in stopwords and re_allpuncspace.search(token) is None and len(token) > 0:\n",
    "                content_pos.append(j)\n",
    "        \n",
    "    if len(slang_pos) == 1:\n",
    "        t = tokens.copy()\n",
    "        for p in slang_pos:\n",
    "            t[p] = '[MASK]'\n",
    "        masked_slang_sents[region_tag].append(re_punc2.sub(add_space_trail, re_punc_space.sub(remove_space, ' '.join(t))).strip())\n",
    "        \n",
    "        t = tokens.copy()\n",
    "        if len(content_pos) > 0:\n",
    "            for p in np.random.choice(len(content_pos), 1, replace=False):\n",
    "                t[content_pos[p]] = '[MASK]'\n",
    "        elif len(tokens) - len(slang_pos) > 0:\n",
    "            p = np.random.choice(len(tokens), 1, replace=False)[0]\n",
    "            while p in slang_pos:\n",
    "                p = np.random.choice(len(tokens), 1, replace=False)[0]\n",
    "            t[p] = '[MASK]'\n",
    "        else:\n",
    "            print(t)\n",
    "        masked_random_sents[region_tag].append(re_punc2.sub(add_space_trail, re_punc_space.sub(remove_space, ' '.join(t))).strip())\n",
    "            \n",
    "        context_sents[region_tag].append(re_punc2.sub(add_space_trail, re_punc_space.sub(remove_space, ' '.join(tokens))).strip())\n",
    "        context_inds[region_tag].append(i)\n",
    "        gt_words[region_tag].append(slang_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1cc32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sample = min([len(s) for s in context_sents])\n",
    "ind_region = slang_llm_inds['region']\n",
    "\n",
    "pivot_tr = int(np.floor(N_sample*0.8))\n",
    "pivot_dev = int(np.floor(N_sample*0.85))\n",
    "\n",
    "tr_sents, dev_sents, te_sents = [], [], []\n",
    "tr_labels, dev_labels, te_labels = [], [], []\n",
    "\n",
    "for i in range(2):\n",
    "    tr_sents.extend(np.asarray(context_sents[i])[ind_region[i]][:pivot_tr])\n",
    "    dev_sents.extend(np.asarray(context_sents[i])[ind_region[i]][pivot_tr:pivot_dev])\n",
    "    te_sents.extend(np.asarray(context_sents[i])[ind_region[i]][pivot_dev:])\n",
    "    \n",
    "    tr_labels.extend([i]*pivot_tr)\n",
    "    dev_labels.extend([i]*(pivot_dev-pivot_tr))\n",
    "    te_labels.extend([i]*(N_sample-pivot_dev))\n",
    "\n",
    "tr_sents = np.asarray(tr_sents)\n",
    "dev_sents = np.asarray(dev_sents)\n",
    "te_sents = np.asarray(te_sents)\n",
    "\n",
    "tr_labels = np.asarray(tr_labels)\n",
    "dev_labels = np.asarray(dev_labels)\n",
    "te_labels = np.asarray(te_labels)\n",
    "\n",
    "# Sentences with masked out slang\n",
    "\n",
    "tr_sents_mslg, dev_sents_mslg, te_sents_mslg = [], [], []\n",
    "\n",
    "for i in range(2):\n",
    "    tr_sents_mslg.extend(np.asarray(masked_slang_sents[i])[ind_region[i]][:pivot_tr])\n",
    "    dev_sents_mslg.extend(np.asarray(masked_slang_sents[i])[ind_region[i]][pivot_tr:pivot_dev])\n",
    "    te_sents_mslg.extend(np.asarray(masked_slang_sents[i])[ind_region[i]][pivot_dev:])\n",
    "    \n",
    "tr_sents_mslg = np.asarray(tr_sents_mslg)\n",
    "dev_sents_mslg = np.asarray(dev_sents_mslg)\n",
    "te_sents_mslg = np.asarray(te_sents_mslg)\n",
    "\n",
    "# Sentences with a random content word (other than the slang) masked out\n",
    "\n",
    "tr_sents_mrand, dev_sents_mrand, te_sents_mrand = [], [], []\n",
    "\n",
    "for i in range(2):\n",
    "    tr_sents_mrand.extend(np.asarray(masked_random_sents[i])[ind_region[i]][:pivot_tr])\n",
    "    dev_sents_mrand.extend(np.asarray(masked_random_sents[i])[ind_region[i]][pivot_tr:pivot_dev])\n",
    "    te_sents_mrand.extend(np.asarray(masked_random_sents[i])[ind_region[i]][pivot_dev:])\n",
    "    \n",
    "tr_sents_mrand = np.asarray(tr_sents_mrand)\n",
    "dev_sents_mrand = np.asarray(dev_sents_mrand)\n",
    "te_sents_mrand = np.asarray(te_sents_mrand)\n",
    "\n",
    "# For RoBERTa and XLNet\n",
    "\n",
    "tr_sents_mslg_alt = np.asarray([s.replace('[MASK]', '<mask>') for s in tr_sents_mslg])\n",
    "dev_sents_mslg_alt = np.asarray([s.replace('[MASK]', '<mask>') for s in dev_sents_mslg])\n",
    "te_sents_mslg_alt = np.asarray([s.replace('[MASK]', '<mask>') for s in te_sents_mslg])\n",
    "\n",
    "tr_sents_mrand_alt = np.asarray([s.replace('[MASK]', '<mask>') for s in tr_sents_mrand])\n",
    "dev_sents_mrand_alt = np.asarray([s.replace('[MASK]', '<mask>') for s in dev_sents_mrand])\n",
    "te_sents_mrand_alt = np.asarray([s.replace('[MASK]', '<mask>') for s in te_sents_mrand])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520c640",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8272a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_demo_classifier(tr_sents, tr_labels, dev_sents, dev_labels, save_path, model_type='bert', num_labels=6, N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False):\n",
    "\n",
    "    if model_type.lower() == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-large-cased', num_labels=num_labels).to(device)\n",
    "        n_save = 4\n",
    "    elif model_type.lower() == 'roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels).to(device)\n",
    "        n_save = 4\n",
    "    elif model_type.lower() == 'xlnet':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-large-cased\")\n",
    "        model = XLNetForSequenceClassification.from_pretrained(\"xlnet-large-cased\", num_labels=num_labels).to(device)\n",
    "        n_save = 4\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return\n",
    "\n",
    "    for name, param in list(model.named_parameters())[:-n_save]:\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5)\n",
    "\n",
    "    best_dev = np.inf\n",
    "    \n",
    "    if verbose:\n",
    "        range_fn = range\n",
    "        range_fn2 = trange\n",
    "    else:\n",
    "        range_fn = trange\n",
    "        range_fn2 = range\n",
    "\n",
    "    for e in range_fn(N_EPOCHS):\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[Training Epoch - %d]\" % (e+1))\n",
    "            print(\"\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        tr_shuf = np.random.permutation(tr_sents.shape[0])\n",
    "\n",
    "        loss_total, loss_total_dev = 0, 0\n",
    "\n",
    "        for b in range_fn2(0, tr_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "            b_end = min(tr_sents.shape[0], b+BATCH_SIZE)\n",
    "            batch_sample = tr_shuf[b:b_end]\n",
    "\n",
    "            tr_sents_batch = tr_sents[batch_sample]\n",
    "            tr_labels_batch = tr_labels[batch_sample]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tr_ids_batch = tokenizer(list(tr_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "            labels = torch.tensor(tr_labels_batch).to(device)\n",
    "\n",
    "            loss = model(**tr_ids_batch, labels=labels).loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total += loss.item() * batch_sample.shape[0]\n",
    "\n",
    "            del tr_ids_batch, labels, loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training Loss: %.3f\" % (loss_total / tr_sents.shape[0]))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        for b in range_fn2(0, dev_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "            b_end = min(dev_sents.shape[0], b+BATCH_SIZE)\n",
    "            batch_sample = np.arange(b, b_end)\n",
    "\n",
    "            dev_sents_batch = dev_sents[batch_sample]\n",
    "            dev_labels_batch = dev_labels[batch_sample]\n",
    "\n",
    "            dev_ids_batch = tokenizer(list(dev_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "            labels = torch.tensor(dev_labels_batch).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss = model(**dev_ids_batch, labels=labels).loss\n",
    "\n",
    "            loss_total_dev += loss.item() * batch_sample.shape[0]\n",
    "\n",
    "            del dev_ids_batch, labels, loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Dev Loss: %.3f\" % (loss_total_dev / dev_sents.shape[0]))\n",
    "        \n",
    "        scheduler.step(loss_total_dev)\n",
    "\n",
    "        if loss_total_dev < best_dev:\n",
    "\n",
    "            best_dev = loss_total_dev\n",
    "            if verbose:\n",
    "                print(\"Best dev loss so far, saving model...\")\n",
    "            d = OrderedDict()\n",
    "            for name, param in list(model.state_dict().items())[-n_save:]:\n",
    "                d[name] = param\n",
    "            torch.save(d, save_path)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def test_demo_classifier(te_sents, save_path, model_type='bert', num_labels=6, BATCH_SIZE = 20):\n",
    "    \n",
    "    if model_type.lower() == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-large-cased', num_labels=num_labels).to(device)\n",
    "    elif model_type.lower() == 'roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels).to(device)\n",
    "    elif model_type.lower() == 'xlnet':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-large-cased\")\n",
    "        model = XLNetForSequenceClassification.from_pretrained(\"xlnet-large-cased\", num_labels=num_labels).to(device)\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return\n",
    "    \n",
    "    _ = model.load_state_dict(torch.load(save_path), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for b in trange(0, te_sents.shape[0], BATCH_SIZE):\n",
    "\n",
    "        b_end = min(te_sents.shape[0], b+BATCH_SIZE)\n",
    "        batch_sample = np.arange(b, b_end)\n",
    "\n",
    "        te_sents_batch = te_sents[batch_sample]\n",
    "\n",
    "        te_ids_batch = tokenizer(list(te_sents_batch), return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(**te_ids_batch).logits.argmax(axis=1)\n",
    "\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "\n",
    "        del te_ids_batch, pred\n",
    "\n",
    "    return np.asarray(preds)\n",
    "\n",
    "def run_demo_experiment(tr_sents, tr_labels, dev_sents, dev_labels, te_sents, te_labels, save_path, num_labels=6, num_exp=20, model_type='bert', N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False):\n",
    "    \n",
    "    res = []\n",
    "    acc = []\n",
    "    \n",
    "    for e in range(num_exp):\n",
    "        print(\"[Experiment - %d]\" % (e+1))\n",
    "        print(\"\")\n",
    "        \n",
    "        path = save_path+'_'+model_type+'_'+str(e+1)+'.pt'\n",
    "        \n",
    "        train_demo_classifier(tr_sents, tr_labels, dev_sents, dev_labels, save_path=path, model_type=model_type, num_labels=num_labels, N_EPOCHS=N_EPOCHS, BATCH_SIZE=BATCH_SIZE, verbose=verbose)\n",
    "        preds = test_demo_classifier(te_sents, path, model_type, num_labels=num_labels)\n",
    "        \n",
    "        res.append(preds)\n",
    "        acc.append(np.sum(preds==te_labels))\n",
    "        \n",
    "    return np.stack(res), np.asarray(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f6bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../Results/region/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226addff",
   "metadata": {},
   "source": [
    "Performance with full sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f083457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment - 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:36<00:00,  3.64s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 15.34it/s]\n"
     ]
    }
   ],
   "source": [
    "res_region_bert, acc = run_demo_experiment(tr_sents, tr_labels, dev_sents, dev_labels, te_sents, te_labels, save_path=save_path+'base', num_exp=1, model_type='bert', num_labels=2, N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8478b3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6170212765957447"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc/len(te_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c6350",
   "metadata": {},
   "source": [
    "Performance with the slang being masked out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33f3ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment - 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:37<00:00,  3.72s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 14.80it/s]\n"
     ]
    }
   ],
   "source": [
    "res_regionmslg_bert, acc = run_demo_experiment(tr_sents_mslg, tr_labels, dev_sents_mslg, dev_labels, te_sents_mslg, te_labels, save_path=save_path+'mslg', num_exp=1, model_type='bert', num_labels=2, N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2559a597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5797872340425532"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc/len(te_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd69d66",
   "metadata": {},
   "source": [
    "Performance with a random content word (non-slang) masked out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5849163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment - 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:36<00:00,  3.63s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 15.70it/s]\n"
     ]
    }
   ],
   "source": [
    "res_regionmrand_bert, acc = run_demo_experiment(tr_sents_mrand, tr_labels, dev_sents_mrand, dev_labels, te_sents_mrand, te_labels, save_path=save_path+'mrand', num_exp=1, model_type='bert', num_labels=2, N_EPOCHS = 10, BATCH_SIZE = 20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a737503a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6117021276595744"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc/len(te_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c904e34",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f48db3",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3427f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mask = data_literal != ''\n",
    "\n",
    "sents_all = data_sents[data_mask]\n",
    "slang_all = data_slang[data_mask]\n",
    "\n",
    "punctuations = '!\\'\"#$%&()\\*\\+,-\\./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "re_punc = re.compile(r\"[\"+punctuations+r\"]+\")\n",
    "re_punc2 = re.compile(r\"[,.!;:&<>-]+\")\n",
    "re_punc_space = re.compile(r\" [\"+punctuations+r\"]+ \")\n",
    "re_space = re.compile(r\" +\")\n",
    "\n",
    "def add_space(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return ' '+match_obj.group()+' '\n",
    "    \n",
    "def add_space_trail(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return match_obj.group()+' '\n",
    "    \n",
    "def remove_space(match_obj):\n",
    "    if match_obj.group() is not None:\n",
    "        return match_obj.group()[1:-1]\n",
    "\n",
    "sents_mask = []\n",
    "for i in range(len(sents_all)):\n",
    "    tokens = re_space.sub(' ', re_punc.sub(add_space, sents_all[i])).split(' ')\n",
    "    slang_pos = []\n",
    "            \n",
    "    for j in range(len(tokens)):\n",
    "        token = tokens[j]\n",
    "        if token.lower() == slang_all[i].lower():\n",
    "            slang_pos.append(j)\n",
    "            \n",
    "    if len(slang_pos) == 0:\n",
    "        tokens = re_space.sub(' ', re_punc.sub('', sents_all[i])).split(' ')\n",
    "        slang_pos = []\n",
    "        slang_nop = re_punc.sub('', slang_all[i])\n",
    "\n",
    "        for j in range(len(tokens)):\n",
    "            token = tokens[j]\n",
    "            if token.lower() == slang_nop.lower():\n",
    "                slang_pos.append(j)\n",
    "        \n",
    "    if len(slang_pos) == 1:\n",
    "        t = tokens.copy()\n",
    "        for p in slang_pos:\n",
    "            t[p] = '[MASK]'\n",
    "        sents_mask.append(re_punc2.sub(add_space_trail, re_punc_space.sub(remove_space, ' '.join(t))).strip())\n",
    "    else:\n",
    "        sents_mask.append('')\n",
    "            \n",
    "sents_mask = np.asarray(sents_mask)\n",
    "\n",
    "pphr_mask = sents_mask != ''\n",
    "sents_mask = sents_mask[pphr_mask]\n",
    "sents_all = sents_all[pphr_mask]\n",
    "words_slang = slang_all[pphr_mask]\n",
    "words_literal = data_literal[data_mask][pphr_mask]\n",
    "regions = data_region[data_mask][pphr_mask]\n",
    "confs = data_conf[data_mask][pphr_mask]\n",
    "\n",
    "sents_mask_pos = np.asarray([sents_mask[i].index('[MASK]') for i in range(len(sents_mask))])\n",
    "sents_mask = np.asarray([sents_mask[i][:sents_mask_pos[i]+6] for i in range(len(sents_mask))])\n",
    "sents_literal = np.asarray([sents_mask[i].replace('[MASK]', words_literal[i])[:sents_mask_pos[i]+len(words_literal[i])] for i in range(len(sents_mask))])\n",
    "sents_slang = np.asarray([sents_mask[i].replace('[MASK]', words_slang[i])[:sents_mask_pos[i]+len(words_slang[i])] for i in range(len(sents_mask))])\n",
    "\n",
    "# Alternative masking token used by RoBERTa and XLNet\n",
    "\n",
    "sents_mask_alt = []\n",
    "for i in range(len(sents_mask)):\n",
    "    sents_mask_alt.append(sents_mask[i].replace('[MASK]', '<mask>'))\n",
    "sents_mask_alt = np.asarray(sents_mask_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d38409b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_mlm = slang_llm_inds['usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb368e",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49bd7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_sents_bert(tokenizer, sents, gt_words, sents_mask_pos):\n",
    "    \n",
    "    id_words = [tokenizer.encode(word)[1:-1] for word in gt_words]\n",
    "\n",
    "    sents_repeat = []\n",
    "    for i in range(len(sents)):\n",
    "        sents_repeat.extend([sents[i] for j in range(len(id_words[i]))])\n",
    "\n",
    "    inputs = tokenizer(list(sents_repeat), return_tensors=\"pt\", padding=True)\n",
    "    input_mask = []\n",
    "\n",
    "    c = 0\n",
    "    for i in range(len(sents)):\n",
    "        m_start = len(tokenizer.encode(sents[i][:sents_mask_pos[i]]))-1\n",
    "        for j in range(len(id_words[i])):\n",
    "            inputs.input_ids[c][m_start+j] = tokenizer.mask_token_id\n",
    "            input_mask.append(i)\n",
    "            c += 1\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    input_mask = np.asarray(input_mask)\n",
    "\n",
    "    id_gt_tokens = []\n",
    "    for i in range(len(sents)):\n",
    "        id_gt_tokens.extend(id_words[i])\n",
    "    id_gt_tokens = np.asarray(id_gt_tokens)\n",
    "    \n",
    "    return inputs, input_mask, id_gt_tokens\n",
    "\n",
    "def preproc_sents_roberta(tokenizer, sents, gt_words, sents_mask_pos):\n",
    "    \n",
    "    id_words = []\n",
    "    for i in range(len(gt_words)):\n",
    "        if sents_mask_pos[i] == 0:\n",
    "            id_words.append(tokenizer.encode(gt_words[i])[1:-1])\n",
    "        else:\n",
    "            id_words.append(tokenizer.encode(' '+gt_words[i])[1:-1])\n",
    "\n",
    "    sents_repeat = []\n",
    "    for i in range(len(sents)):\n",
    "        sents_repeat.extend([sents[i] for j in range(len(id_words[i]))])\n",
    "\n",
    "    inputs = tokenizer(list(sents_repeat), return_tensors=\"pt\", padding=True)\n",
    "    input_mask = []\n",
    "\n",
    "    c = 0\n",
    "    for i in range(len(sents)):\n",
    "        m_start = len(tokenizer.encode(sents[i][:sents_mask_pos[i]].strip()))-1\n",
    "        for j in range(len(id_words[i])):\n",
    "            inputs.input_ids[c][m_start+j] = tokenizer.mask_token_id\n",
    "            input_mask.append(i)\n",
    "            c += 1\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    input_mask = np.asarray(input_mask)\n",
    "\n",
    "    id_gt_tokens = []\n",
    "    for i in range(len(sents)):\n",
    "        id_gt_tokens.extend(id_words[i])\n",
    "    id_gt_tokens = np.asarray(id_gt_tokens)\n",
    "    \n",
    "    return inputs, input_mask, id_gt_tokens\n",
    "\n",
    "def preproc_sents_xlnet(tokenizer, sents, gt_words, sents_mask_pos):\n",
    "    \n",
    "    id_words = [tokenizer.encode(word)[:-2] for word in gt_words]\n",
    "\n",
    "    sents_repeat = []\n",
    "    for i in range(len(sents)):\n",
    "        sents_repeat.extend([sents[i] for j in range(len(id_words[i]))])\n",
    "\n",
    "    inputs = tokenizer(list(sents_repeat), return_tensors=\"pt\", padding=True)\n",
    "    input_mask = []\n",
    "\n",
    "    c = 0\n",
    "    for i in range(len(sents)):\n",
    "        m_start = 0\n",
    "        while inputs.input_ids[c][m_start] == 5:\n",
    "            m_start += 1\n",
    "        m_start += len(tokenizer.encode(sents[i][:sents_mask_pos[i]]))-2\n",
    "        for j in range(len(id_words[i])):\n",
    "            inputs.input_ids[c][m_start+j] = tokenizer.mask_token_id\n",
    "            input_mask.append(i)\n",
    "            c += 1\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    input_mask = np.asarray(input_mask)\n",
    "\n",
    "    id_gt_tokens = []\n",
    "    for i in range(len(sents)):\n",
    "        id_gt_tokens.extend(id_words[i])\n",
    "    id_gt_tokens = np.asarray(id_gt_tokens)\n",
    "    \n",
    "    return inputs, input_mask, id_gt_tokens\n",
    "\n",
    "\n",
    "def predict_logits_bert(model, inputs):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    softmax = torch.nn.functional.softmax(logits[torch.arange(logits.shape[0]), mask_token_index], dim=1)\n",
    "    \n",
    "    return softmax\n",
    "\n",
    "def predict_mlm_bert(logits, id_gt_tokens):\n",
    "    \n",
    "    return logits[torch.arange(logits.shape[0]), id_gt_tokens]\n",
    "\n",
    "def predict_logits_roberta(model, inputs):\n",
    "    \n",
    "    return predict_logits_bert(model, inputs)\n",
    "\n",
    "def predict_mlm_roberta(logits, id_gt_tokens):\n",
    "    \n",
    "    return predict_mlm_bert(logits, id_gt_tokens)\n",
    "\n",
    "def predict_logits_xlnet(model, inputs):\n",
    "\n",
    "    perm_mask = torch.zeros((inputs.input_ids.shape[0], inputs.input_ids.shape[1], inputs.input_ids.shape[1]), dtype=torch.float).to(device)\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        i_pad = 0\n",
    "        while inputs.input_ids[i][i_pad]==5:\n",
    "            i_pad += 1\n",
    "        perm_mask[i, :, list(inputs.input_ids[i].flatten().cpu().numpy()).index(6)] = 1\n",
    "        perm_mask[i, :i_pad, :] = 1\n",
    "        perm_mask[i, :, :i_pad] = 1\n",
    "\n",
    "    target_mapping = torch.zeros((inputs.input_ids.shape[0], 1, inputs.input_ids.shape[1]), dtype=torch.float).to(device)\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        target_mapping[i, 0, list(inputs.input_ids[i].flatten().cpu().numpy()).index(6)] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs, perm_mask=perm_mask, target_mapping=target_mapping).logits\n",
    "\n",
    "    softmax = torch.nn.functional.softmax(logits, dim=2)\n",
    "    \n",
    "    return softmax\n",
    "\n",
    "def predict_mlm_xlnet(logits, id_gt_tokens):\n",
    "    \n",
    "    return logits[torch.arange(logits.shape[0]), 0, id_gt_tokens]\n",
    "\n",
    "def predict_mlm_batch(model, tokenizer, sents, id_gt_tokens, model_type='bert', BATCH_SIZE=100):\n",
    "    \n",
    "    if model_type.lower() == 'bert':\n",
    "        logit_fn = predict_logits_bert\n",
    "        mlm_fn = predict_mlm_bert\n",
    "    elif model_type.lower() == 'roberta':\n",
    "        logit_fn = predict_logits_roberta\n",
    "        mlm_fn = predict_mlm_roberta\n",
    "    elif model_type.lower() == 'xlnet':\n",
    "        logit_fn = predict_logits_xlnet\n",
    "        mlm_fn = predict_mlm_xlnet\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return\n",
    "\n",
    "    N_ex = len(sents)\n",
    "    results = torch.empty(N_ex, dtype=torch.float64).to(device)\n",
    "\n",
    "    for b in trange(0, N_ex, BATCH_SIZE):\n",
    "\n",
    "        b_end = min(N_ex, b+BATCH_SIZE)\n",
    "\n",
    "        logits = logit_fn(model, tokenizer, sents[b:b_end])\n",
    "        results[b:b_end] = mlm_fn(logits, tokenizer, id_gt_tokens[b:b_end])\n",
    "        \n",
    "    return results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc1c14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-large-cased\").to(device)\n",
    "\n",
    "inputs_literal, input_mask_literal, id_gt_tokens_literal = preproc_sents_bert(tokenizer, sents_literal, words_literal, sents_mask_pos)\n",
    "inputs_slang, input_mask_slang, id_gt_tokens_slang = preproc_sents_bert(tokenizer, sents_slang, words_slang, sents_mask_pos)\n",
    "\n",
    "logits_literal = predict_logits_bert(model, inputs_literal)\n",
    "prob_literal = predict_mlm_bert(logits_literal, id_gt_tokens_literal).cpu().numpy()\n",
    "prob_literal = np.asarray([np.mean(prob_literal[input_mask_literal==i]) for i in range(len(sents_mask_pos))])\n",
    "\n",
    "logits_slang = predict_logits_bert(model, inputs_slang)\n",
    "prob_slang = predict_mlm_bert(logits_slang, id_gt_tokens_slang).cpu().numpy()\n",
    "prob_slang = np.asarray([np.mean(prob_slang[input_mask_slang==i]) for i in range(len(sents_mask_pos))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ae78a",
   "metadata": {},
   "source": [
    "Mean ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0626537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2493036"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prob_slang[ind_mlm]) / np.mean(prob_literal[ind_mlm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e4980",
   "metadata": {},
   "source": [
    "Median ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3000d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75862646"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(prob_slang[ind_mlm] / prob_literal[ind_mlm])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
